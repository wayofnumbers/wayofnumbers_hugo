---
title: "Came across this piece talking about the current machinelearning status by Mark Saroufim."
author: "Michael Li"
date: 2021-01-27T06:36:59.767Z
lastmod: 2021-04-18T00:05:49-05:00

description: ""

subtitle: ""

image: "2021-01-27_came-across-this-piece-talking-about-the-current-sharpmachinelearning-status-by-mark-saroufim._0.jpeg" 
images:
 - "2021-01-27_came-across-this-piece-talking-about-the-current-sharpmachinelearning-status-by-mark-saroufim._0.jpeg"


aliases:
- "came-across-this-piece-talking-about-the-current-machinelearning-status-by-mark-saroufim-6936f0aa51a4"

---

Came across this piece talking about **the current**[**#machinelearning**](https://twitter.com/search?q=%23machinelearning "#machinelearning")**status by**[**Mark Saroufim**](https://medium.com/u/426d54390627)**.**Some views are a bit controversial but good points nonetheless.

[Machine Learning: The Great Stagnation](https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation "https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation")

Iâ€™ll summarize the key points below: ğŸ§µğŸ‘‡

1. ML Researchersâ€Šâ€”â€ŠSupposed to be risk-taking and less commercial oriented so ground-breaking progress can be made. Rather, ML Researchers found ways to not taking any risk but getting good pay through FANNG, media, YouTube, etc., and SOTA chasing.
2. Math is overrated in Deep Learning. Matrix multiplication is mostly what you need and auto grad removes the real needs for manual gradient calculation. Be real now.
3. The empiricism tendency of DL formed a feedback loop, where people/company has more computing power to do many experiments in parallel wins, thus gaining more resources to do even more experiments. Google Brain, OpenAI, DeepMind.
4. Transformers are everywhere and so popular
5. Graduate Student Descent. Again, more people to do experiments leads to faster breakthroughs. Also, cargo-culting configs, loss functions, frameworks make it possible but less well-thought-out.
6. Good innovation in ML:
    â€Šâ€”â€ŠKeras, fast.ai: user-centric, layered in nature, software engineering for ML is a good direction.
    â€Šâ€”â€ŠJuliaâ€Šâ€”â€ŠDifferentiable Computing from the ground up. (Swift may also be)
    â€Šâ€”â€ŠHuggingFaceâ€Šâ€”â€Šso popular NLP model
    â€Šâ€”â€ŠHasktorchâ€Šâ€”â€ŠHaskell based, elegant, less known, and developed
    â€Šâ€”â€ŠUnity ML agentsâ€Šâ€”â€Šgreat for RLÂ 
    â€Šâ€”â€ŠAlphaFold2 is groundbreaking

On another note, this maybe can explain why China is getting stronger in AI. It has abundant computing power, many aspiring graduate students, mass media market for AI experts, plus â€˜unlimitedâ€™ data due to loose privacy regulations. Make one thinkâ€¦ ğŸ¤·â€â™‚ï¸ğŸ¤”ğŸ¤”

* * *
Written on January 27, 2021 by Michael Li.

Originally published on [Medium](https://medium.com/@lymenlee/came-across-this-piece-talking-about-the-current-machinelearning-status-by-mark-saroufim-6936f0aa51a4)
